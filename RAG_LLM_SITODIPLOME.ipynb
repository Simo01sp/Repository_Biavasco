{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simo01sp/Repository_Biavasco/blob/main/RAG_LLM_SITODIPLOME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNleFP8DAvm1",
        "outputId": "f5c841fe-55fa-46ca-ad76-a4efaa28505f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Crawling del sito in corso...\n",
            "üìÑ Trovate 23 pagine.\n",
            "\n",
            "‚úÇÔ∏è Creazione chunk...\n",
            "üîπ Creati 438 chunk.\n",
            "\n",
            "üìò Carico DB_QA.xlsx...\n",
            "üìö Indicizzate 180 domande FAQ\n",
            "üï∏Ô∏è Indicizzati 438 chunk del sito\n",
            "\n",
            "ü§ñ Chatbot attivo! Scrivi una domanda.\n",
            "\n",
            "\n",
            "‚ùì FAQ Question Found: None\n",
            "üìò FAQ Score: 0.0\n",
            "üìÑ Website Score: 0.35085880756378174\n",
            "üîó Website Sources:\n",
            "   - https://cimea-diplome.it/\n",
            "   - https://cimea-diplome.it/page-homepage\n",
            "   - https://cimea-diplome.it\n",
            "\n",
            "Bot: On the DiploMe platform, there are three different services available. Your qualification can be related to only one of these services. To determine the type of service relevant to your qualification and the required documents, please ensure that you accurately select the Country of Education System and the Level of Education System in the Service Finder.\n",
            "\n",
            "‚ùì FAQ Question Found: How much do CIMEA services cost?\n",
            "üìò FAQ Score: 0.47026458382606506\n",
            "üìÑ Website Score: 0.29611802101135254\n",
            "üîó Website Sources:\n",
            "   - https://cimea-diplome.it/page-faq&p=1\n",
            "   - https://cimea-diplome.it/page-faq\n",
            "   - https://cimea-diplome.it/page-comparability-verification-documents\n",
            "\n",
            "Bot: The costs for CIMEA services on the DiploMe platform vary depending on the type of service requested. For each individual qualification and related documents, the ordinary procedure costs ‚Ç¨300.00 (which includes ‚Ç¨245.90 plus VAT of ‚Ç¨54.10). If you require the urgent procedure for a Comparability request, there is an additional fee of ‚Ç¨100.00 (which comprises ‚Ç¨81.97 plus VAT of ‚Ç¨18.03). Please note that Verification requests are always processed according to the ordinary procedure. For more details on available services and their related costs, we recommend using the dedicated tool or visiting the relevant webpage for comprehensive information.\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "#   CIMEA CHATBOT ‚Äî FULL RAG WITH LLM FAQ FILTER\n",
        "# =====================================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "\n",
        "# =====================================================\n",
        "#   CONFIGURAZIONE\n",
        "# =====================================================\n",
        "\n",
        "BASE_URL = \"https://cimea-diplome.it\"\n",
        "#API_KEY set individually\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   1 ‚Äî CRAWLING DEL SITO\n",
        "# =====================================================\n",
        "\n",
        "def crawl_website(base_url):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    pages = []\n",
        "\n",
        "    print(\"\\nüîç Crawling del sito in corso...\")\n",
        "\n",
        "    while to_visit:\n",
        "        url = to_visit.pop()\n",
        "\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            res = requests.get(url, timeout=10)\n",
        "            if res.status_code != 200:\n",
        "                continue\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        pages.append({\"url\": url, \"text\": page_text})\n",
        "\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            full_url = urljoin(url, link[\"href\"])\n",
        "            if base_url in full_url and full_url not in visited:\n",
        "                to_visit.append(full_url)\n",
        "\n",
        "    print(f\"üìÑ Trovate {len(pages)} pagine.\")\n",
        "    return pages\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   2 ‚Äî CHUNKING\n",
        "# =====================================================\n",
        "\n",
        "def chunk_pages(pages, chunk_size=500):\n",
        "    chunks = []\n",
        "    print(\"\\n‚úÇÔ∏è Creazione chunk...\")\n",
        "\n",
        "    for p in pages:\n",
        "        text = p[\"text\"]\n",
        "        for i in range(0, len(text), chunk_size):\n",
        "            chunk = text[i:i + chunk_size]\n",
        "            chunks.append({\n",
        "                \"url\": p[\"url\"],\n",
        "                \"chunk\": chunk\n",
        "            })\n",
        "\n",
        "    print(f\"üîπ Creati {len(chunks)} chunk.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   3 ‚Äî EMBEDDING MODEL\n",
        "# =====================================================\n",
        "\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_texts(text_list):\n",
        "    return embedding_model.encode(\n",
        "        text_list,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   4 ‚Äî FAQ (EXCEL)\n",
        "# =====================================================\n",
        "\n",
        "def load_faq_embeddings():\n",
        "    print(\"\\nüìò Carico DB_QA.xlsx...\")\n",
        "\n",
        "    df = pd.read_excel(\"DB_QA.xlsx\")\n",
        "    df = df.rename(columns={\n",
        "        \"Domanda\": \"question\",\n",
        "        \"Answer (ENGLISH)\": \"answer\"\n",
        "    })\n",
        "\n",
        "    faq_questions = df[\"question\"].tolist()\n",
        "    faq_embeddings = embed_texts(faq_questions)\n",
        "\n",
        "    dim = faq_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(faq_embeddings)\n",
        "\n",
        "    print(f\"üìö Indicizzate {index.ntotal} domande FAQ\")\n",
        "    return df, index\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   5 ‚Äî WEBSITE INDEX\n",
        "# =====================================================\n",
        "\n",
        "def build_website_index(chunks):\n",
        "    chunk_texts = [c[\"chunk\"] for c in chunks]\n",
        "    chunk_embeddings = embed_texts(chunk_texts)\n",
        "\n",
        "    dim = chunk_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(chunk_embeddings)\n",
        "\n",
        "    print(f\"üï∏Ô∏è Indicizzati {index.ntotal} chunk del sito\")\n",
        "    return index\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   6 ‚Äî LLM FILTER: VERIFICA PERTINENZA DELLA FAQ\n",
        "# =====================================================\n",
        "\n",
        "def is_faq_relevant(user_question, faq_question):\n",
        "    if faq_question is None:\n",
        "        return False\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a classifier.\n",
        "\n",
        "Given a USER QUESTION and a FAQ QUESTION, answer ONLY with \"yes\" or \"no\".\n",
        "\n",
        "USER QUESTION:\n",
        "{user_question}\n",
        "\n",
        "FAQ QUESTION:\n",
        "{faq_question}\n",
        "\n",
        "Is the FAQ QUESTION relevant?\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=5,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    verdict = response.choices[0].message.content.strip().lower()\n",
        "    return verdict == \"yes\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   7 ‚Äî RICERCA MISTA (FAQ + SITO)\n",
        "# =====================================================\n",
        "\n",
        "def ricerca_mista(user_question, faq_df, faq_index, chunks, index_chunks):\n",
        "\n",
        "    user_emb = embed_texts([user_question])\n",
        "\n",
        "    # --- FAQ ---\n",
        "    sim_faq, idx_faq = faq_index.search(user_emb, k=1)\n",
        "    faq_answer = faq_df.iloc[idx_faq[0][0]][\"answer\"]\n",
        "    faq_question_found = faq_df.iloc[idx_faq[0][0]][\"question\"]\n",
        "    faq_score = float(sim_faq[0][0])\n",
        "\n",
        "    # üîç FILTRO LLM: scarta FAQ se non pertinente\n",
        "    if not is_faq_relevant(user_question, faq_question_found):\n",
        "        faq_answer = None\n",
        "        faq_question_found = None\n",
        "        faq_score = 0.0\n",
        "\n",
        "    # --- WEBSITE ---\n",
        "    sim_sito, idx_sito = index_chunks.search(user_emb, k=3)\n",
        "    sito_chunks = [chunks[i][\"chunk\"] for i in idx_sito[0]]\n",
        "    sito_sources = [chunks[i][\"url\"] for i in idx_sito[0]]\n",
        "    sito_score = float(sim_sito[0][0])\n",
        "\n",
        "    return {\n",
        "        \"faq_answer\": faq_answer,\n",
        "        \"faq_question\": faq_question_found,\n",
        "        \"faq_score\": faq_score,\n",
        "\n",
        "        \"sito_chunks\": sito_chunks,\n",
        "        \"sito_score\": sito_score,\n",
        "        \"sito_sources\": sito_sources\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   8 ‚Äî RISPOSTA LLM\n",
        "# =====================================================\n",
        "\n",
        "def answer_with_llm(question, data):\n",
        "\n",
        "    faq_answer = data[\"faq_answer\"]\n",
        "    sito_chunks = \"\\n\".join(data[\"sito_chunks\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "User question:\n",
        "{question}\n",
        "\n",
        "FAQ information:\n",
        "{faq_answer}\n",
        "\n",
        "Website information:\n",
        "{sito_chunks}\n",
        "\n",
        "Write a final answer that:\n",
        "- is clear, professional and helpful\n",
        "- uses ONLY the information above\n",
        "- does NOT invent facts\n",
        "- MUST be written in English\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=600   # risposta completa\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   9 ‚Äî MAIN CHAT LOOP\n",
        "# =====================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    pages = crawl_website(BASE_URL)\n",
        "    chunks = chunk_pages(pages)\n",
        "    faq_df, faq_index = load_faq_embeddings()\n",
        "    index_chunks = build_website_index(chunks)\n",
        "\n",
        "    print(\"\\nü§ñ Chatbot attivo! Scrivi una domanda.\\n\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nYou: \")\n",
        "        if q.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        info = ricerca_mista(q, faq_df, faq_index, chunks, index_chunks)\n",
        "\n",
        "        print(\"\\n‚ùì FAQ Question Found:\", info[\"faq_question\"])\n",
        "        print(\"üìò FAQ Score:\", info[\"faq_score\"])\n",
        "        print(\"üìÑ Website Score:\", info[\"sito_score\"])\n",
        "\n",
        "        print(\"üîó Website Sources:\")\n",
        "        for s in info[\"sito_sources\"]:\n",
        "            print(\"   -\", s)\n",
        "\n",
        "        answer = answer_with_llm(q, info)\n",
        "        print(\"\\nBot:\", answer)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "#   AVVIO\n",
        "# =====================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzFApNcvBOqt",
        "outputId": "06bf6962-0fa0-4ae0-ba6a-cdaea641b708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas sentence-transformers faiss-cpu openai numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6uIFG5IdkM8v"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3YF6fFjH5BOenRFCSJxQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}